### 이 질문에 대해서 생각해보기 전에 컴퓨터가 개발되던 시기로 돌아가보자.

컴퓨터는 미국에서 발전한 기계인데, 처음 컴퓨터를 통해 문자를 나타내기 위한 규약인 ASCII(American Standard Code for Information Interchange)도 알파벳을 포함한 미국에서 사용하기 위한 문자들을 표현하기 위해 만든 것이었다.

ASCII는 초창기 문자 집합 중 하나로, 영어 알파벳과 아라비아 숫자, 그리고 일부 특수문자를 포함한다. 

이때 ASCII의 총 문자수가 공백을 포함하여 128자였기 때문에 7bit 만으로 모든 문자를 표현하는 것이 가능했다.

![https://blog.kakaocdn.net/dn/qOPNt/btrAdcY26CF/Ksn1qKzUqEaCql1Cbk6GG0/img.png](https://blog.kakaocdn.net/dn/qOPNt/btrAdcY26CF/Ksn1qKzUqEaCql1Cbk6GG0/img.png)

### **🤔 그럼 7bit여야하는거 아닌가?**

싶지만 8비트 중 1비트는 패리티 비트라고 불리는 오류 검출을 위해 사용되기 때문에 1바이트가 8비트로 이루어져 있는 것이다


1바이트는 8비트다라고 당연하게 생각하고 넘어갔던 것들에 대해서 궁금해하고 찾아보니 새로운 정보들을 찾아보게 되고 새롭게 알게 된 것들이 많아 보람차다.
이후에는 문자를 인코딩하는 방법에 대해서 알아보자 (๑・‿・๑)

